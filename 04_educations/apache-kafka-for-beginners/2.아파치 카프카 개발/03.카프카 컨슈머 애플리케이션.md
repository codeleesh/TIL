# 카프카 컨슈머 애플리케이션

- 컨슈머는 기본적으로 토픽의 데이터를 가져옴
  - 데이터는 토픽 내부에 파티션에 저장
- 컨슈머는 파티션에 저장된 데이터를 가져옴
- 데이터를 가져오는 것을 폴링이라고함

## 컨슈머의 역할

- Topic의 partition으로부터 데이터 polling
- Partition offset 위치 기록(commit)
- Consumer group을 통해 병렬처리

## 카프카 사용

- 카프카 클라이언트와 카프카 브로러 호환 버전 가능한지 확인 필요

```
compile group: 'org.apache.kafka', name: 'kafka-clients', version: '2.3.0'
```

## 컨슈머

```java
public class Consumer {
  public static void main(String[] args) {
    Properties configs = new Properties();
    configs.put("bootstrap.servers", "localhost:9092")
    configs.put("group.id", "click_log_group")
    configs.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    configs.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")

    KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(configs);

    consumer.subscribe(Arrays.asList("click_log"));

    while (true) {
      ConsumerRecords<String, String> records = consumer.poll(500);
      for (ConsumerRecord<String, String> record : records) {
        System.out.println(record.value());
      }
    }
  }
}
```
  - `configs.put("bootstrap.servers", "localhost:9092")`
    - 카프카 브로커 설정
    - 카프카 브로커 중 한개에 이슈가 생기면 다른 브로커가 붙을 수 있도록 여러개의 브로커를 설정하는 것을 추천
  - `configs.put("group.id", "click_log_group")`
    - 그룹 아이디를 지정
    - 그룹 아이디는 컨슈머 그룹이라고도 불림
  - `configs.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")`
    - key, value 에 대한 직렬화 설정
  - `configs.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")`
    - key, value 에 대한 직렬화 설정
  - `KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(configs);`
    - 컨슈머 인스턴스 생성
    - 컨슈머 인스턴스를 통해 데이터를 읽고 처리할 수 있음
  - `consumer.subscribe(Arrays.asList("click_log"));`
    - 어느 토픽에서 데이터를 가져올지 설정

  ```java
  TopicPartition partition0 = new TopicPartition(topicName, 0);
  TopicPartition partition1 = new TopicPartition(topicName, 1);
  consumer.assign(Arrays.asList(partition0, partition1));
  ```
  - 특정 토픽의 전체 파티션이 아니라 일부 파티션의 데이터만 가져오고 싶다면 assign() 메서드 사용
  - key가 존재하는 데이터라면 이 방식을 통해 데이터의 순서를 보장하는 데이터 처리 가능

  ```java
  while (true) {
        ConsumerRecords<String, String> records = consumer.poll(500);
        for (ConsumerRecord<String, String> record : records) {
          System.out.println(record.value());
        }
      }
  ```
  - 폴링 루프 구문  
    - poll 메서드가 포함된 무한 루프 구문
  - 컨슈머 API의 핵심은 브로커로부터 연속적으로, 그리고 컨슈머가 허락하는 한 많은 데이터를 읽는 것
  - 0.5초 동안 데이터가 도착하기를 기다리고 이후 코드를 실행
  - 만약 0.5초 동안 데이터가 들어오지 않으면 빈 값의 records 변수를 반환하고 데이터가 있다면 데이터가 존재하는 records 값을 반환
    - records 변수는 데이터 배치로서 레코드의 묶음 list

## Consuemer.poll()

컨슈머가 데이터를 읽기 시작하면 offset을 commit하게 되는데, 이렇게 가져간 내용에 대한 정보는 카프카의 __consumer_offset 토픽에 저장
  - consumer가 정보를 가져갈때마다 offset 정보가 저장
  - 컨슈머가 불의의 사고로 실행이 중지되었다면 중지전까지 읽었던 정보의 offset 정보 저장
  - 컨슈머에 이슈가 발생하더라도 데이터의 처리 시점을 복구할 수 있는 고 가용성의 특징을 가지게됨

## Multiple Consumer

- 컨슈머는 몇개까지 생성할 수 있을까요? (파티션이 2개인 경우)
  - 컨슈머가 1개인 경우 2개의 파티션에서 데이터를 가져감
  - 2개의 컨슈머인 경우 각 컨슈머가 각각의 파티션을 할당하여 데이터를 가져가서 처리
  - 3개의 컨슈머인 경우?
    - 파티션들이 각 컨슈머에 할당되었기 때문에 더이상 할당될 파티션이 없어서 동작하지 않음
    - 컨슈머 개수는 파티션 개수보다 적거나 같아야함
  - 여러 파티션을 가진 토픽에 대해서 컨슈머를 병렲처리하고 싶다면 반드시 컨슈머를 파티션 개수보다 적은 개수로 실행시켜야 한다는 점을 꼭 기억

## Different groups

- 각기 다른 컨슈머 그룹에 속한 컨슈머들은 다른 컨슈머 그룹에 영향을 미치지 않음
- __consumer_offset 토픽에서는 컨슈머 그룹별로 토픽별로 offset을 나누어 저장하기 때문
